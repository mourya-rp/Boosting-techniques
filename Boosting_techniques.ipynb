{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Techniques\n",
        "\n",
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "- Boosting is a machine learning technique that combines multiple weak learners into a single, strong model by training them sequentially. It improves weak learners by iteratively focusing on the errors made by previous models, giving more weight to misclassified instances in subsequent steps. This process continues until the desired level of accuracy is reached, transforming simple, flawed models into a powerful predictor.\n",
        "\n",
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "- AdaBoost and Gradient Boosting differ in how they handle errors: AdaBoost increases the weights of misclassified samples to make the next model focus on them, while Gradient Boosting fits new models to the residuals (errors) of the previous ensemble. Additionally, AdaBoost uses weak learners like decision stumps and weights their final contribution based on performance, whereas Gradient Boosting can use more complex learners and adds them to the ensemble in a greedy, forward, stage-wise manner to minimize a specific loss function.\n",
        "\n",
        "3. How does regularization help in XGBoost?\n",
        "\n",
        "- Regularization in XGBoost primarily helps to prevent overfitting and improve the generalization ability of the model. XGBoost, being a powerful tree-boosting algorithm, can easily overfit the training data if not properly regularized, especially with complex trees or noisy datasets.\n",
        "\n",
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "- CatBoost handles categorical data efficiently by using its Ordered Target Encoding and Ordered Boosting techniques to prevent target leakage, and by using oblivious (symmetric) trees that simplify splits and speed up training. These innovations allow CatBoost to natively process categorical features, reducing the need for manual pre-processing and often improving model performance compared to other methods that rely on one-hot encoding or require manual preprocessing.\n",
        "\n",
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "- Boosting is preferred in real-world applications where a high degree of accuracy is needed to reduce bias, particularly in complex tasks like fraud detection, customer churn prediction, and financial forecasting. Boosting works by sequentially building models that focus on the mistakes of previous models, making it effective for problems with a complex underlying relationship that bagging, which primarily reduces variance, may not handle as well.\n",
        "\n",
        "Applications favoring boosting\n",
        "\n",
        "Fraud detection: Boosting models can achieve high accuracy in identifying complex patterns of fraudulent activity, especially when dealing with massive datasets.\n",
        "\n",
        "Customer churn prediction: The sequential nature of boosting allows it to learn from the data to accurately predict which customers are most likely to leave.\n",
        "\n",
        "Financial forecasting and pricing: Boosting can be used in finance to build highly accurate models for tasks like stock market prediction and the pricing of complex financial products.\n",
        "\n",
        "Image recognition: In scenarios like identifying cats in images, boosting can combine simple, weak learners (e.g., a weak learner that looks for pointy ears and another that looks for cat-shaped eyes) into a strong, accurate classifier, as explained by Amazon Web Services (AWS).\n",
        "\n",
        "\n",
        "Why boosting is preferred in these cases\n",
        "\n",
        "Reduces bias: Boosting focuses on reducing bias by sequentially training models that focus on the errors of the previous ones, making it ideal for complex problems.\n",
        "\n",
        "High accuracy: By iteratively improving on previous models, boosting often achieves higher predictive accuracy than bagging, though it requires more careful tuning to avoid overfitting.\n",
        "\n",
        "Handles complex data: Boosting is particularly effective on datasets with relatively few predictors but complex relationships between them.\n",
        "When bagging might be a better choice\n",
        "\n",
        "Improves stability: Bagging reduces variance and is great for unstable models, making it effective when the goal is to improve model stability rather than just accuracy.\n",
        "\n",
        "Handles high-variance models: Applications like Random Forests use bagging to effectively handle high-variance, low-bias models.\n",
        "\n",
        "Reduces overfitting: Bagging's parallel training approach is robust to overfitting, which is useful when the primary concern is preventing the model from fitting too closely to the training data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jNeuhs2715Te"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vvNWD4D1x85",
        "outputId": "51a18502-daa3-4426-ff42-9b262cadf1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Model Accuracy: 0.9708\n"
          ]
        }
      ],
      "source": [
        "#6. Write a Python program to:\n",
        "#Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "#Print the model accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\n",
        "\n",
        "\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Model Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to:\n",
        "#Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "#Evaluate performance using R-squared score\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor R-squared score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54xRiqf03J9w",
        "outputId": "16db8d5d-511a-49a9-a1b7-2915ec38b676"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to:\n",
        "#Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "#Tune the learning rate using GridSearchCV\n",
        "#Print the best parameters and accuracy\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with best parameters: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vwv4UIR3K0s",
        "outputId": "334d7db9-7f2b-43b2-dcad-d64be120ffb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best parameters: {'learning_rate': 0.2}\n",
            "Best cross-validation accuracy: 0.9670\n",
            "Test set accuracy with best parameters: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [14:32:58] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "#Train a CatBoost Classifier\n",
        "#Plot the confusion matrix using seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "\n",
        "X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "y_series = pd.Series(y, name='target')\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y_series, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = CatBoostClassifier(iterations=100,\n",
        "                           learning_rate=0.1,\n",
        "                           depth=6,\n",
        "                           loss_function='Logloss',\n",
        "                           eval_metric='Accuracy',\n",
        "                           random_seed=42,\n",
        "                           verbose=False)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "_xoXtUAp3Kyh",
        "outputId": "70498439-d209-44df-b24e-9a2f45b160d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2748520788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2df07caf",
        "outputId": "37a4015d-b004-488b-b707-7e0231e08c9f"
      },
      "source": [
        "%pip install catboost"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "f7b4916f",
        "outputId": "88d8a1ab-524d-4c0e-bdd5-ab091d04511b"
      },
      "source": [
        "#9. Write a Python program to:\n",
        "#Train a CatBoost Classifier\n",
        "#Plot the confusion matrix using seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "\n",
        "X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "y_series = pd.Series(y, name='target')\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y_series, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = CatBoostClassifier(iterations=100,\n",
        "                           learning_rate=0.1,\n",
        "                           depth=6,\n",
        "                           loss_function='Logloss',\n",
        "                           eval_metric='Accuracy',\n",
        "                           random_seed=42,\n",
        "                           verbose=False)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "Data preprocessing & handling missing/categorical values\n",
        "\n",
        "Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "Hyperparameter tuning strategy\n",
        "\n",
        "Evaluation metrics you'd choose and why\n",
        "\n",
        "How the business would benefit from your model\n",
        "\n",
        "\n",
        "\n",
        "A step-by-step data science pipeline for predicting loan defaults using boosting techniques on an imbalanced dataset with missing values and mixed features is described below.\n",
        "\n",
        "1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "The preprocessing stage focuses on data cleaning, feature engineering, and addressing the class imbalance issue.\n",
        "\n",
        "Data Cleaning and Feature Engineering: Conduct exploratory data analysis (EDA) to understand data distributions, outliers, and correlations. Create relevant features from existing data (e.g., debt-to-income ratio, loan-to-income ratio, age of credit history).\n",
        "\n",
        "Handling Missing Values:\n",
        "\n",
        "For XGBoost and LightGBM, missing values can be left as NaN (the default), as these algorithms have a sparsity-aware split finding and can learn the best direction for a split when a value is missing.\n",
        "\n",
        "For CatBoost, missing values need to be explicitly imputed or handled, as it cannot handle them natively in all scenarios. Imputation methods like mean/median for numerical features and mode for categorical features can be used.\n",
        "\n",
        "Handling Categorical Values:\n",
        "\n",
        "XGBoost requires categorical features to be converted to numeric format, typically using one-hot encoding for low cardinality features or other methods like target encoding for high cardinality features.\n",
        "\n",
        "LightGBM supports categorical features natively if they are integer-encoded and specified using a cat_features parameter.\n",
        "\n",
        "CatBoost has superior native handling of categorical features using an efficient ordered target encoding method, which helps prevent data leakage and improves performance.\n",
        "\n",
        "Handling Imbalance: The target variable (loan default) is highly imbalanced. This can be addressed using a combination of methods:\n",
        "\n",
        "Resampling Techniques: Employ synthetic oversampling techniques like SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN on the training data.\n",
        "\n",
        "Class Weighting: Utilize the scale_pos_weight parameter in XGBoost or similar class weights in other models during model training to give more importance to the minority (default) class.\n",
        "\n",
        "2. Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "CatBoost or XGBoost are the recommended choices over AdaBoost for this problem.\n",
        "\n",
        "CatBoost is an excellent choice due to its state-of-the-art handling of categorical features natively, which can be a significant advantage given the dataset description. It often requires less extensive preprocessing for these features.\n",
        "\n",
        "XGBoost is a highly efficient, regularized gradient boosting framework known for its speed and performance, making it a popular choice in FinTech and Kaggle competitions.\n",
        "\n",
        "AdaBoost is generally less performant than modern gradient boosting methods like XGBoost or CatBoost for complex, large datasets with mixed data types.\n",
        "\n",
        "The final choice between CatBoost and XGBoost would depend on empirical testing (performance, training time, memory usage) on the specific dataset.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Hyperparameter tuning is crucial for optimizing the performance of boosting models, especially with imbalanced data.\n",
        "\n",
        "Tuning Method: Use advanced search techniques like Random Search or Bayesian Optimization to efficiently explore the hyperparameter space, as a full Grid Search can be computationally expensive.\n",
        "\n",
        "Cross-Validation: Implement k-fold cross-validation on the training data to ensure robust model evaluation and to prevent overfitting to a single train/validation split.\n",
        "\n",
        "Key Parameters to Tune:\n",
        "\n",
        "Class Imbalance: Tune scale_pos_weight (XGBoost/LightGBM) or class weights (CatBoost) to find the optimal balance between precision and recall for the minority class.\n",
        "\n",
        "Tree Complexity: Parameters like max_depth, num_leaves, min_child_weight (or similar in other libraries) control tree complexity and help prevent overfitting.\n",
        "\n",
        "Regularization: Tune L1 (reg_alpha) and L2 (reg_lambda) regularization terms to make the model more conservative.\n",
        "\n",
        "Learning Rate and Number of Estimators: Tune learning_rate and n_estimators (or num_boost_round) to balance training speed and model performance.\n",
        "\n",
        "4. Evaluation Metrics You'd Choose and Why\n",
        "\n",
        "Accuracy is misleading for imbalanced datasets. The following metrics should be used:\n",
        "\n",
        "Precision-Recall AUC (PR-AUC): This is the primary metric because it focuses on the performance of the positive (default) class, which is rare but critical to identify.\n",
        "\n",
        "F1-Score (or F-beta score): The F1-score provides a harmonic mean of precision and recall. A business may prioritize recall over precision (e.g., higher penalty for missing a default than for a false alarm), in which case an F2-score would be more appropriate.\n",
        "\n",
        "ROC AUC: While PR-AUC is preferred for severe imbalance, ROC AUC still offers a good overall measure of the model's ability to distinguish between classes across all possible thresholds and is widely used for comparison.\n",
        "\n",
        "Confusion Matrix: Used to understand the raw counts of True Positives, True Negatives, False Positives, and False Negatives to inform business decisions on the classification threshold.\n",
        "\n",
        "5. How the Business Would Benefit from Your Model\n",
        "\n",
        "The model provides significant business value by enabling proactive risk management and informed decision-making.\n",
        "\n",
        "Improved Risk Management: Accurately identifying potential defaulters early allows the company to intervene, offer modified payment plans, or deny loans to high-risk applicants, thereby reducing financial losses.\n",
        "Enhanced Profitability: By minimizing loan defaults and managing risk effectively, the company can optimize its loan portfolio and ensure financial stability.\n",
        "\n",
        "Automated and Consistent Decisions: The model provides a consistent, data-driven approach to loan approvals, reducing human bias and improving operational efficiency.\n",
        "\n",
        "Optimized Resource Allocation: The business can focus collection efforts on the accounts most likely to default, rather than treating all overdue accounts the same way.\n"
      ],
      "metadata": {
        "id": "KBq-fkcR5Q6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, auc, roc_auc_score, confusion_matrix, make_scorer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "# from imblearn.over_sampling import SMOTE # Uncomment if using SMOTE instead of class weights\n",
        "# from imblearn.pipeline import Pipeline as ImbPipeline # Uncomment if using ImbPipeline\n",
        "\n",
        "# --- 1. Data Preprocessing & Handling Missing/Categorical Values ---\n",
        "\n",
        "def preprocess_data(df):\n",
        "    if 'loan_default' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'loan_default' column.\")\n",
        "\n",
        "    X = df.drop('loan_default', axis=1)\n",
        "    y = df['loan_default']\n",
        "\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    numeric_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ], remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    return X, y, preprocessor\n",
        "\n",
        "# --- 2. Choice between AdaBoost, XGBoost, or CatBoost ---\n",
        "\n",
        "def get_model():\n",
        "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    return model\n",
        "\n",
        "# --- 3. Hyperparameter Tuning Strategy ---\n",
        "\n",
        "def tune_hyperparameters(X_train_processed, y_train):\n",
        "    param_dist = {\n",
        "        'n_estimators': [100, 200, 300], # Added example values\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7], # Added example values\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'scale_pos_weight': [1, (len(y_train) - sum(y_train)) / sum(y_train)]\n",
        "    }\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('model', get_model())\n",
        "    ])\n",
        "\n",
        "    pr_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        scoring=pr_auc_scorer,\n",
        "        cv=skf,\n",
        "        verbose=1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    random_search.fit(X_train_processed, y_train)\n",
        "    return random_search.best_estimator_\n",
        "\n",
        "# --- 4. Evaluation Metrics ---\n",
        "\n",
        "def evaluate_model(model, X_test_processed, y_test):\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "# --- Main script ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        data = {\n",
        "            'feature1': np.random.rand(500),\n",
        "            'feature2': np.random.choice(['A', 'B', 'C'], 500),\n",
        "            'loan_default': np.random.choice([0, 1], 500, p=[0.9, 0.1])\n",
        "        }\n",
        "        loan_data = pd.DataFrame(data)\n",
        "        loan_data.iloc[10:50, 0] = np.nan\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dummy data: {e}\")\n",
        "        exit()\n",
        "\n",
        "    X, y, preprocessor = preprocess_data(loan_data)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    best_model_pipeline = tune_hyperparameters(X_train_processed, y_train)\n",
        "    print(\"Tuning complete.\")\n",
        "    print(f\"Best parameters found: {best_model_pipeline.named_steps['model'].get_params()}\")\n",
        "\n",
        "    print(\"\\nEvaluating the best model on test data...\")\n",
        "    evaluate_model(best_model_pipeline, X_test_processed, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzfcJn_c3KtW",
        "outputId": "0f59c780-a702-48c4-af1a-6959670fb630"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter tuning...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Tuning complete.\n",
            "Best parameters found: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 1.0, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.2, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 7, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': 10.428571428571429, 'subsample': 1.0, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}\n",
            "\n",
            "Evaluating the best model on test data...\n",
            "ROC AUC: 0.4420\n",
            "PR AUC: 0.0840\n",
            "F1 Score: 0.1111\n",
            "Confusion Matrix:\n",
            "[[83  8]\n",
            " [ 8  1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [14:45:21] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3ixrFpS3Kqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}